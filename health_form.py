# app.py
import os
import io
import tempfile
import hashlib
import pandas as pd
import streamlit as st

from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import InMemoryVectorStore
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()
RECIPE_GROQ_API_KEY = os.environ.get("RECIPE_GROQ_API_KEY")
GROQ_MODEL = os.environ.get("GROQ_MODEL", "deepseek-r1-distill-llama-70b")

if not RECIPE_GROQ_API_KEY:
    st.error("Missing GROQ API key. Set RECIPE_GROQ_API_KEY in your environment.")
    st.stop()

st.markdown("""
<style>
.stApp { background-color: #0E1117; color: #FFFFFF; }
.stFileUploader { background-color: #1E1E1E; border: 1px solid #3A3A3A; border-radius: 5px; padding: 15px; }
h1, h2, h3 { color: #FFF00 !important; }
.block-label { color:#a8b3cf; font-size:0.9rem; }
</style>
""", unsafe_allow_html=True)


PROMPT_TEMPLATE = """
You are a personalized meal planning AI assistant.
Use the provided recipe dataset and the user's health profile to generate recommendations.

*User Health Profile:*
{health_profile}

*Query:*
{user_query}

*Relevant Recipes (top matches):*
{document_context}

Provide, in order:
1) Nutritional Analysis of the recommended meals.
2) Dietary Guidelines tailored to the user's profile.
3) Ingredient Substitutions (for allergies/restrictions/preferences).
4) 3â€“5 Meal Recommendations with short justifications tied to the health profile.

Be concise but specific. If data is insufficient, state assumptions explicitly.
"""

@st.cache_resource(show_spinner=False)
def get_embedder():
    return HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

@st.cache_resource(show_spinner=False)
def get_llm():
    return ChatGroq(
        groq_api_key=RECIPE_GROQ_API_KEY,
        model_name=GROQ_MODEL,
        temperature=0.3,
        max_tokens=1024,
    )

@st.cache_resource(show_spinner=False)
def get_vectorstore(_embedding):
    return InMemoryVectorStore(_embedding)


def _hash_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()

@st.cache_data(show_spinner=False)
def bytes_to_tempfile(file_bytes: bytes, suffix: str) -> str:
    """Persist uploaded bytes into a temp file and return its path."""
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(file_bytes)
        return tmp.name

@st.cache_data(show_spinner=False)
def load_as_documents(tmp_path: str, is_csv: bool, sample_rows: int | None = None) -> list[Document]:
    """
    Robust doc loader that uses pandas with encoding fallbacks and returns LangChain Documents.
    - Skips bad lines.
    - Optional sampling for very large files.
    """
    if is_csv:
        try:
            df = pd.read_csv(tmp_path, encoding="utf-8", on_bad_lines="skip")
        except UnicodeDecodeError:
            df = pd.read_csv(tmp_path, encoding="ISO-8859-1", on_bad_lines="skip")
    else:
        df = pd.read_excel(tmp_path)

    if sample_rows is not None and len(df) > sample_rows:
        df = df.sample(n=sample_rows, random_state=13).reset_index(drop=True)

    docs = [
        Document(
            page_content=" | ".join(map(str, row)),
            metadata={"row_index": int(idx)}
        )
        for idx, row in df.iterrows()
    ]
    return docs

def chunk_documents(documents: list[Document], chunk_size=1000, chunk_overlap=200):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True
    )
    return splitter.split_documents(documents)

def index_documents(vstore: InMemoryVectorStore, chunks: list[Document]):
    batch_size = 64
    prog = st.progress(0, text="Indexing embeddings...")
    for i in range(0, len(chunks), batch_size):
        vstore.add_documents(chunks[i:i+batch_size])
        prog.progress(min(100, int((i + batch_size) / max(1, len(chunks)) * 100)))
    prog.progress(100)

def build_context(vstore: InMemoryVectorStore, query: str, k: int = 4, per_doc_chars: int = 800) -> str:
    hits = vstore.similarity_search(query, k=k)
    trimmed = []
    for d in hits:
        txt = d.page_content
        trimmed.append(txt[:per_doc_chars])
    return "\n\n".join(trimmed)

def generate_response(llm: ChatGroq, health_profile: str, user_query: str, context: str) -> str:
    prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    chain = prompt | llm
    out = chain.invoke(
        {
            "health_profile": health_profile,
            "user_query": user_query,
            "document_context": context,
        }
    )
    return getattr(out, "content", out)

if "health_profile" not in st.session_state:
    st.session_state.health_profile = None

if "file_hash" not in st.session_state:
    st.session_state.file_hash = None

if "dataset_loaded" not in st.session_state:
    st.session_state.dataset_loaded = False

if "vector_ready" not in st.session_state:
    st.session_state.vector_ready = False

embedding_model = get_embedder()
lang_model = get_llm()
vector_store = get_vectorstore(embedding_model)

st.title("ðŸ©ºDiabetes Risk Predictor")
st.markdown("### Predict diabetes risk based on your health data")
st.markdown("---")

with st.form("health_form"):

    st.subheader("ðŸ§¾ Enter Your Health Data")
    pregnancies = st.number_input("Pregnancies", min_value=0, max_value=20, step=1)
    glucose = st.number_input("Glucose Level", min_value=0, max_value=300, step=1)
    bp = st.number_input("Blood Pressure", min_value=0, max_value=200, step=1)
    skin_thickness = st.number_input("Skin Thickness", min_value=0, max_value=100, step=1)
    insulin = st.number_input("Insulin", min_value=0, max_value=900, step=1)
    bmi = st.number_input("BMI", min_value=0.0, max_value=70.0, step=0.1)
    dpf = st.number_input("Diabetes Pedigree Function", min_value=0.0, max_value=3.0, step=0.01)
    age = st.number_input("Age", min_value=0, max_value=120, step=1)

    st.markdown("<span class='block-label'>Upload your recipe dataset (CSV or Excel)</span>", unsafe_allow_html=True)
    recipe_file = st.file_uploader(" ", type=["csv", "xlsx"], label_visibility="collapsed")

    col1, col2, col3 = st.columns(3)
    save_profile_btn = col1.form_submit_button("ðŸ’¾ Save Profile")
    process_dataset_btn = col2.form_submit_button("ðŸ“¤ Process Dataset")
    generate_btn = col3.form_submit_button("ðŸ¤– Generate Plan")

if save_profile_btn:
    hp = f"""Pregnancies: {pregnancies}
Glucose Level: {glucose}
Blood Pressure: {bp}
Skin Thickness: {skin_thickness}
Insulin: {insulin}
BMI: {bmi}
Diabetes Pedigree Function: {dpf}
Age: {age}
"""
    st.markdown("### ðŸ§¾ Saved Health Profile")
    st.text(hp)
    st.session_state.health_profile = hp
    st.success("âœ… Health profile saved!")

if process_dataset_btn:
    if not recipe_file:
        st.error("Please upload a recipe dataset first.")
    else:
        file_bytes = recipe_file.getvalue()
        st.session_state.file_hash = _hash_bytes(file_bytes)
        suffix = os.path.splitext(recipe_file.name)[1] or ".csv"

        with st.spinner("Saving uploaded file and parsing..."):
            tmp_path = bytes_to_tempfile(file_bytes, suffix)
            is_csv = recipe_file.name.lower().endswith(".csv")

        SAMPLE_ROWS = None 

        with st.spinner("Converting rows to documents..."):
            documents = load_as_documents(tmp_path, is_csv=is_csv, sample_rows=SAMPLE_ROWS)

        with st.spinner("Chunking documents..."):
            chunks = chunk_documents(documents, chunk_size=1000, chunk_overlap=200)

        with st.spinner("Embedding & indexing (batched)..."):
            index_documents(vector_store, chunks)

        st.session_state.dataset_loaded = True
        st.session_state.vector_ready = True
        st.success(f"âœ… Dataset processed and indexed! Chunks: {len(chunks)}")


if generate_btn:
    if not st.session_state.health_profile:
        st.error("Health_Plan.")
    elif not st.session_state.vector_ready:
        st.error("Please process your  Health Information.")
    else:
        st.markdown("---")
        st.subheader("ðŸ’¬ Ask for  Health Information  ")
        default_query = "Plan meals for the next 3 days considering my allergies and health conditions."
        user_query = st.text_input("Your query", value=default_query)

        if user_query.strip():
            with st.spinner("Retrieving context..."):
                context = build_context(vector_store, user_query, k=4, per_doc_chars=800)

            with st.spinner("Generating recommendations..."):
                result = generate_response(
                    lang_model, st.session_state.health_profile, user_query, context
                )

            st.markdown("### ðŸ¤– Health Care System ")
            st.write(result)


with st.expander("âš¡ Tips to keep it fast"):
    st.markdown(
        "- Use CSVs with fewer than ~50k rows while prototyping.\n"
        "- Consider setting SAMPLE_ROWS to cap rows for very large files.\n"
        "- Keep queries focused; the retriever feeds only the top matches to the LLM.\n"
        "- You can re-upload a dataset and click *Process Dataset* to rebuild theÂ index.")